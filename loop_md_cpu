#!/bin/bash -l

# Script for running multidir Gromacs simulations on LUMI in loops.
# Part of RoVa Lab LUMI environment.
# Author: Ladislav Bartos
# Version 0.2

# @@@@@@@@@@@@@@@@@@@@@@@@@@
# @     SLURM OPTIONS      @
# @@@@@@@@@@@@@@@@@@@@@@@@@@

#SBATCH --job-name=loop_md
#SBATCH --account=project_465000590
#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=32
#SBATCH --mem=0
#SBATCH --partition=standard
#SBATCH -e job-%j.err -o job-%j.out

# `nodes` corresponds to the number of nodes to use; LUMI CPU nodes have 128 CPU cores
# `ntasks` corresponds to the number of MPI ranks to use PER NODE
# `cpus-per-task` corresponds to the number of OpenMP threads to use per MPI rank

# It seems that on LUMI it is optimal to use 1 MPI rank per simulation.

# @@@@@@@@@@@@@@@@@@@@@@@@@@
# @     GROMACS OPTIONS    @
# @@@@@@@@@@@@@@@@@@@@@@@@@@

# name of the directory to be used for data storage (inside each simulation directory)
STORAGE_DIR="storage"

# pattern for simulation directory names
SIM_DIRS="run"

# GROMACS FILES TO USE
MDP="umbrella.mdp"
GRO="system.gro"
CPT=""  # leave empty if not needed
TOP="system.top"
NDX="index.ndx"
REF=""  # leave empty if not needed
MAXWARN="1"

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@

# get current project id
if [[ $(pwd) =~ /scratch/project_([0-9]+) ]]; then
  PROJECT_ID="${BASH_REMATCH[1]}"
else
  echo "ERROR. Could not identify project ID. Are you submiting the simulation from project SCRATCH storage?"
  exit 1
fi

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export EBU_USER_PREFIX=/project/project_${PROJECT_ID}/EasyBuild

module load LUMI/22.08
module load partition/L

module load GROMACS/2021.4-cpeGNU-22.08-PLUMED-2.7.4-cray-python-3.9.12.1-CPU

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# get the current cycle of the loop job
if [ -f finished_cycle ]; then
    CYCLE=$(( $(cat finished_cycle) + 1 ))
else
    CYCLE=1
fi

# get the next cycle of the loop job
NEXT=$(( $CYCLE + 1 ))

CYCLE_FMT=$(printf "%04d" ${CYCLE})
NEXT_FMT=$(printf "%04d" ${NEXT})

# if this is the first cycle, run grompp for each window
if [ $CYCLE -eq 1 ]; then
    for SIM in ${SIM_DIRS}*; do
        # but first prepare STORAGE_DIR for every window
        if [ ! -d ${SIM}/${STORAGE_DIR} ]; then mkdir ${SIM}/${STORAGE_DIR}; fi
     
        # optional checkpoint and reference files for grompp
        if [ ! -z ${REF} ]; then FULL_REF="-r ${SIM}/${REF}"; fi
        if [ ! -z ${CPT} ]; then FULL_CPT="-t ${SIM}/${CPT}"; fi

        gmx grompp -f ${SIM}/${MDP} -c ${SIM}/${GRO} ${FULL_CPT} -p ${SIM}/${TOP} -n ${SIM}/${NDX} ${FULL_REF} -o ${SIM}/${STORAGE_DIR}/md${CYCLE_FMT}.tpr -maxwarn ${MAXWARN} || exit 1
    done
    CPI_COMMAND=""
else
    CPI_COMMAND="-cpi ${STORAGE_DIR}/md${CYCLE_FMT}.cpt"
fi

# run the gromacs simulation
srun -n $(( $SLURM_NTASKS * $SLURM_NNODES )) gmx_mpi mdrun -ntomp $SLURM_CPUS_PER_TASK -v -deffnm ${STORAGE_DIR}/md${CYCLE_FMT} ${CPI_COMMAND} -cpo ${STORAGE_DIR}/md${NEXT_FMT} -multidir $(ls -d ${SIM_DIRS}* | sort -h) -noappend -pin on || exit 1 

for SIM in ${SIM_DIRS}*; do
    cd $SIM
    
    # extend the simulation
    EXTEND=`awk 'BEGIN {dt = 0.001; nsteps = 0} /nsteps/ {nsteps = $3} /dt/ {dt = $3} END {ext = nsteps * dt; if (!ext) {exit 2} else {print ext}}' ${MDP}`
    if [ $? -ne 0 ]; then exit 1; fi
    gmx convert-tpr -s ${STORAGE_DIR}/md${CYCLE_FMT}.tpr -extend ${EXTEND} -o ${STORAGE_DIR}/md${NEXT_FMT}.tpr -quiet || exit 1
    
    # rename the output files
    for FILE_SUFFIX in .gro .edr .log _pullf.xvg _pullx.xvg .xtc; do
        OLD=${STORAGE_DIR}/md${CYCLE_FMT}.part${CYCLE_FMT}${FILE_SUFFIX}
        if [ -f $OLD ]; then
            mv $OLD ${STORAGE_DIR}/md${CYCLE_FMT}${FILE_SUFFIX}
        fi
     done
    
    cd ..
done

# remove the sub_job file generated by loop_sub
rm -f sub_job_$(echo $SLURM_JOB_ID | tr -dc '0-9')

# save the current cycle of the loop job
# (has to be done at the end of the script execution)
echo $CYCLE > finished_cycle

exit 0
