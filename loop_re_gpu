#!/bin/bash -l

# Script for running Gromacs simulations with Hamiltonian replica exchange on LUMI-G in loops.
# Part of RoVa Lab LUMI environment.
# Author: Ladislav Bartos
# Version 0.1

# WARNING! The number of simulations (windows) must be HALF of the requested number of GPUs!

# @@@@@@@@@@@@@@@@@@@@@@@@@@
# @     SLURM OPTIONS      @
# @@@@@@@@@@@@@@@@@@@@@@@@@@

#SBATCH --job-name=loop_re
#SBATCH --account=project_465000573
#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=8
#SBATCH --gpus-per-node=8
#SBATCH --mem=0
#SBATCH --partition=standard-g
#SBATCH --hint=nomultithread
#SBATCH -e job-%j.err -o job-%j.out

# @@@@@@@@@@@@@@@@@@@@@@@@@@
# @     GROMACS OPTIONS    @
# @@@@@@@@@@@@@@@@@@@@@@@@@@

# name of the directory to be used for data storage (inside each simulation directory)
STORAGE_DIR="storage"

# pattern for simulation directory names
SIM_DIRS="win"

# GROMACS FILES TO USE
MDP="umbrella.mdp"
GRO="system.gro"
CPT=""  # leave empty if not needed
TOP="system.top"
NDX="index.ndx"
REF=""  # leave empty if not needed
MAXWARN="1"

# frequency of HREX exchange attempts
HREX_FREQ=50000

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@
# @@@@@@@@@@@@@@@@@@@@@@@@@@@@

# get current project id
if [[ $(pwd) =~ /scratch/project_([0-9]+) ]]; then
  PROJECT_ID="${BASH_REMATCH[1]}"
else
  echo "ERROR. Could not identify project ID. Are you submiting the simulation from project SCRATCH storage?"
  exit 1
fi

export EBU_USER_PREFIX=/project/project_${PROJECT_ID}/EasyBuild

module load LUMI/22.08
module load partition/G

module load craype-accel-amd-gfx90a
module load rocm

module load GROMACS/2021.4-cpeGNU-22.08-PLUMED-2.8.0-cray-python-3.9.12.1-GPU

export OMP_NUM_THREADS=7
export OMP_PLACES=cores
export OMP_PROC_BIND=close

export GMX_ENABLE_DIRECT_GPU_COMM=1
export GMX_FORCE_GPU_AWARE_MPI=1
export MPICH_GPU_SUPPORT_ENABLED=1

cat << EOF > select_gpu
#!/bin/bash

export ROCR_VISIBLE_DEVICES=\$SLURM_LOCALID
exec \$*
EOF

chmod +x ./select_gpu

# CPU-GPU binding for 7 CPU cores per simulation
CPU_BIND="mask_cpu:fe000000000000,fe00000000000000,fe0000,fe000000,fe,fe00,fe00000000,fe0000000000"

# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# get the current cycle of the loop job
if [ -f finished_cycle ]; then
    CYCLE=$(( $(cat finished_cycle) + 1 ))
else
    CYCLE=1
fi

# get the next cycle of the loop job
NEXT=$(( $CYCLE + 1 ))

CYCLE_FMT=$(printf "%04d" ${CYCLE})
NEXT_FMT=$(printf "%04d" ${NEXT})

# if this is the first cycle, run grompp for each window
if [ $CYCLE -eq 1 ]; then
    for SIM in ${SIM_DIRS}*; do
        # but first prepare STORAGE_DIR for every window
        if [ ! -d ${SIM}/${STORAGE_DIR} ]; then mkdir ${SIM}/${STORAGE_DIR}; fi
     
        # optional checkpoint and reference files for grompp
        if [ ! -z ${REF} ]; then FULL_REF="-r ${SIM}/${REF}"; fi
        if [ ! -z ${CPT} ]; then FULL_CPT="-t ${SIM}/${CPT}"; fi

        gmx grompp -f ${SIM}/${MDP} -c ${SIM}/${GRO} ${FULL_CPT} -p ${SIM}/${TOP} -n ${SIM}/${NDX} ${FULL_REF} -o ${SIM}/${STORAGE_DIR}/md${CYCLE_FMT}.tpr -maxwarn ${MAXWARN} || exit 1
        # prepare plumed file
        touch ${SIM}/plumed.dat
    done
    CPI_COMMAND=""
else
    CPI_COMMAND="-cpi ${STORAGE_DIR}/md${CYCLE_FMT}.cpt"
fi

# run the gromacs simulation
srun -n $(( $SLURM_NTASKS * $SLURM_NNODES )) --cpu-bind=${CPU_BIND} ./select_gpu gmx_mpi mdrun -nb gpu -ntomp ${OMP_NUM_THREADS} -deffnm ${STORAGE_DIR}/md${CYCLE_FMT} ${CPI_COMMAND} -cpo ${STORAGE_DIR}/md${NEXT_FMT} -multidir $(ls -d ${SIM_DIRS}* | sort -h) -noappend -replex ${HREX_FREQ} -hrex -plumed plumed.dat || exit 1

for SIM in ${SIM_DIRS}*; do
    cd $SIM
    
    # extend the simulation
    EXTEND=`awk 'BEGIN {dt = 0.001; nsteps = 0} /nsteps/ {nsteps = $3} /dt/ {dt = $3} END {ext = nsteps * dt; if (!ext) {exit 2} else {print ext}}' ${MDP}`
    if [ $? -ne 0 ]; then exit 1; fi
    gmx convert-tpr -s ${STORAGE_DIR}/md${CYCLE_FMT}.tpr -extend ${EXTEND} -o ${STORAGE_DIR}/md${NEXT_FMT}.tpr -quiet || exit 1
    
    # rename the output files
    for FILE_SUFFIX in .gro .edr .log _pullf.xvg _pullx.xvg .xtc; do
        OLD=${STORAGE_DIR}/md${CYCLE_FMT}.part${CYCLE_FMT}${FILE_SUFFIX}
        if [ -f $OLD ]; then
            mv $OLD ${STORAGE_DIR}/md${CYCLE_FMT}${FILE_SUFFIX}
        fi
     done
    
    cd ..
done

# remove the sub_job file generated by loop_sub
rm -f sub_job_$(echo $SLURM_JOB_ID | tr -dc '0-9')

# save the current cycle of the loop job
# (has to be done at the end of the script execution)
echo $CYCLE > finished_cycle

exit 0
